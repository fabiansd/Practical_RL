{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning problems with crossentropy method. We'll follow-up by scaling everything up and using neural network policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb.\n",
      "env: DISPLAY=: 1\n"
     ]
    }
   ],
   "source": [
    "# In Google Colab, uncomment this:\n",
    "# !wget https://bit.ly/2FMJP5K -O setup.py && bash setup.py\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "174\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "s = env.reset()\n",
    "render = env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\" % (n_states, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.ones([n_states, n_actions]) / n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray, np.matrix)\n",
    "assert np.allclose(policy, 1./n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(policy, t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        a = np.random.choice(n_actions)\n",
    "\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # Record state, action and add up reward to states,actions and total_reward accordingly.\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r = generate_session(policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float, np.float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0dcd3be9b0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see the initial reward distribution\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(policy, t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy method steps (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you are confused, see examples below. Please don't assume that states are integers\n",
    "    (they will become different later).\n",
    "    \"\"\"\n",
    "\n",
    "    reward_threshold = np.percentile(rewards_batch, q=percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    \n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] >= reward_threshold:\n",
    "\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "\n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "states_batch = [\n",
    "    [1, 2, 3],     # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1],        # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],     # game1\n",
    "    [3, 2, 0, 1],  # game2\n",
    "    [3, 3],        # game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,  # game1\n",
    "    4,  # game2\n",
    "    5,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_40 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "test_result_90 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "test_result_100 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "    and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "    np.all(test_result_40[1] == [3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "    np.all(test_result_90[1] == [3, 3]),\\\n",
    "    \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and\\\n",
    "    np.all(test_result_100[1] == [3, 3]),\\\n",
    "    \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros([n_states, n_actions])\n",
    "\n",
    "    <Your code here: update probabilities for actions given elite states & actions >\n",
    "    # Don't forget to set 1/n_actions for all actions in unvisited states.\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_states = [1, 2, 3, 4, 2, 0, 2, 3, 1]\n",
    "elite_actions = [0, 2, 4, 3, 2, 0, 1, 3, 3]\n",
    "\n",
    "new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(\n",
    "), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(\n",
    "    new_policy >= 0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(\n",
    "    axis=-1), 1), \"Your new policy should be a valid probability distribution over actions\"\n",
    "reference_answer = np.array([\n",
    "    [1.,  0.,  0.,  0.,  0.],\n",
    "    [0.5,  0.,  0.,  0.5,  0.],\n",
    "    [0.,  0.33333333,  0.66666667,  0.,  0.],\n",
    "    [0.,  0.,  0.,  0.5,  0.5]])\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset policy just in case\n",
    "policy = np.ones([n_states, n_actions]) / n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sessions = 250  # sample this many sessions\n",
    "percentile = 50  # take this percent of session with highest rewards\n",
    "learning_rate = 0.5  # add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    %time sessions = [ < generate a list of n_sessions new sessions > ]\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "\n",
    "    elite_states, elite_actions = <select elite states/actions >\n",
    "\n",
    "    new_policy = <compute new policy >\n",
    "\n",
    "    policy = learning_rate*new_policy + (1-learning_rate)*policy\n",
    "\n",
    "    # display results on chart\n",
    "    show_progress(rewards_batch, log, percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### You're not done yet!\n",
    "\n",
    "Go to [`./deep_crossentropy_method.ipynb`](./deep_crossentropy_method.ipynb) for a more serious task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
